{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import json\n",
    "import importlib\n",
    "\n",
    "import google_article_search\n",
    "import newsExtractor\n",
    "import processDoc\n",
    "import gensimLDA\n",
    "import wikitags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runSub(pathToStanCoreNLP):\n",
    "    currDir = os.getcwd()\n",
    "    os.chdir(pathToStanCoreNLP)\n",
    "    command = \"java -Xmx5g -cp '*' edu.stanford.nlp.pipeline.StanfordCoreNLP \"\\\n",
    "            + \"-annotators tokenize,ssplit,pos,lemma,ner,parse,mention,coref -coref.algorithm neural -file out.txt -outputFormat json\"\n",
    "    subprocess.check_output(command, shell=True)\n",
    "    os.chdir(currDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def putTextInFile(article, pathToStanCoreNLP):\n",
    "    currDir = os.getcwd()\n",
    "    os.chdir(pathToStanCoreNLP)\n",
    "    try:\n",
    "        with codecs.open(\"out.txt\", \"w\", \"utf-8\") as outFile:\n",
    "            outFile.write(article)\n",
    "    except:\n",
    "        os.chdir(currDir)\n",
    "        print (sys.exc_info())\n",
    "        return False\n",
    "    os.chdir(currDir)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numPosts = 20\n",
    "numComments = 200\n",
    "metaFileName = \"data/metadata.txt\"\n",
    "commentFilenames = [\"comment\" + str(i) for i in range(1,numPosts + 1)]\n",
    "\n",
    "googleApiKey = \"AIzaSyAvnSZCKCeHSZCWVNfQMLvq5XJiOMYYa88\"\n",
    "googleCseID = \"006733671097832492705:vknyjefr9aa\"\n",
    "numWebDocs = 10\n",
    "\n",
    "pathToStanCoreNLP = \"/Users/yashchandarana/Desktop/DM_project/stanford-corenlp-full-2016-10-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metafile = open(metaFileName, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importlib.reload(newsExtractor)\n",
    "importlib.reload(google_article_search)\n",
    "importlib.reload(processDoc)\n",
    "importlib.reload(wikitags)\n",
    "from newsExtractor import extractArticle\n",
    "from google_article_search import google_article_search\n",
    "from processDoc import replaceCorefs, cleanDoc\n",
    "from wikitags import getAnchorTags\n",
    "\n",
    "for currPostNum in range(1, numPosts + 1):\n",
    "    metadata = metafile.readline().split(\";;\")\n",
    "    articleLink = metadata[1]\n",
    "    articleTitle = metadata[2]\n",
    "    properNouns = metadata[3]\n",
    "    articleContent = extractArticle(articleLink)\n",
    "    webArticleContent = {}\n",
    "    for i, link in enumerate(google_article_search(articleTitle, googleApiKey, googleCseID, num=numWebDocs)):\n",
    "        try:\n",
    "            webArtMeta = extractArticle(link)\n",
    "            webArticleContent[i + 1] = webArtMeta[0]\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    articleData = {}\n",
    "    \n",
    "    putTextInFile(articleContent, pathToStanCoreNLP)\n",
    "    runSub(pathToStanCoreNLP)\n",
    "    NLPppn, doc, numSenten = replaceCorefs(\"out.txt.json\", pathToStanCoreNLP)\n",
    "    doc = cleanDoc(doc)\n",
    "    \n",
    "    properNouns = list(set(properNouns + NLPppn))\n",
    "    wikiProperNoun = getAnchorTags(properNouns)\n",
    "    properNouns = list(set(properNouns + wikiProperNoun))\n",
    "    \n",
    "    articleData[0] = [properNouns, doc, numSenten]\n",
    "    \n",
    "    for idx, content in webArticleContent.items():\n",
    "        putTextInFile(content, pathToStanCoreNLP)\n",
    "        runSub(pathToStanCoreNLP)\n",
    "        NLPppn, doc, numSenten = replaceCorefs(\"out.txt.json\", pathToStanCoreNLP)\n",
    "        wikiProperNoun = getAnchorTags(NLPppn)\n",
    "        properNouns = list(set(NLPppn + wikiProperNoun))\n",
    "        articleData[idx] = [properNouns, doc, numSenten]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
