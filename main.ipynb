{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import json\n",
    "import importlib\n",
    "\n",
    "import google_article_search\n",
    "import newsExtractor\n",
    "import processDoc\n",
    "import wikitags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def runSub(pathToStanCoreNLP):\n",
    "    currDir = os.getcwd()\n",
    "    os.chdir(pathToStanCoreNLP)\n",
    "    command = \"java -Xmx5g -cp '*' edu.stanford.nlp.pipeline.StanfordCoreNLP \"\\\n",
    "            + \"-annotators tokenize,ssplit,pos,lemma,ner,parse,mention,coref -coref.algorithm neural -file out.txt -outputFormat json\"\n",
    "    subprocess.check_output(command, shell=True)\n",
    "    os.chdir(currDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def putTextInFile(article, pathToStanCoreNLP):\n",
    "    currDir = os.getcwd()\n",
    "    os.chdir(pathToStanCoreNLP)\n",
    "    try:\n",
    "        with codecs.open(\"out.txt\", \"w\", \"ascii\", errors=\"ignore\") as outFile:\n",
    "            outFile.write(article)\n",
    "    except:\n",
    "        os.chdir(currDir)\n",
    "        print (sys.exc_info())\n",
    "        return False\n",
    "    os.chdir(currDir)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "numPosts = 20\n",
    "numComments = 200\n",
    "metaFileName = \"data/metadata.txt\"\n",
    "commentFilenames = [\"comment\" + str(i) + \".txt\" for i in range(1,numPosts + 1)]\n",
    "\n",
    "googleApiKey = \"AIzaSyAvnSZCKCeHSZCWVNfQMLvq5XJiOMYYa88\"\n",
    "googleCseID = \"006733671097832492705:vknyjefr9aa\"\n",
    "numWebDocs = 10\n",
    "\n",
    "pathToStanCoreNLP = \"/home/bhavana/sem6/final/stanford-corenlp-full-2016-10-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "metafile = open(metaFileName, \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "8\n",
      "8\n",
      "8\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-42888c773645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mrunSub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathToStanCoreNLP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mNLPppn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumSenten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplaceCorefs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"out.txt.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathToStanCoreNLP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/yashchandarana/Desktop/DM_project/processDoc.py\u001b[0m in \u001b[0;36mreplaceCorefs\u001b[0;34m(filename, pathToStanCoreNLP)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msenten\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msenten\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mproperNouns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msenten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "importlib.reload(newsExtractor)\n",
    "importlib.reload(google_article_search)\n",
    "importlib.reload(processDoc)\n",
    "importlib.reload(wikitags)\n",
    "from newsExtractor import extractArticle\n",
    "from google_article_search import google_search\n",
    "from processDoc import replaceCorefs, cleanDoc\n",
    "from wikitags import getAnchorTags\n",
    "\n",
    "# format of per line in metadata => sr. no, link, title, facebook person tags\n",
    "for currPostNum in range(1, numPosts + 1):\n",
    "    metadata = metafile.readline().split(\";;\")\n",
    "    articleLink = metadata[1]\n",
    "    articleTitle = metadata[2]\n",
    "    properNouns = metadata[3].split()\n",
    "    try:\n",
    "        articleContent = extractArticle(articleLink)\n",
    "    except Exception as e:\n",
    "        print (str(e))\n",
    "    webArticleContent = {}\n",
    "    for i, link in enumerate(google_search(articleTitle, googleApiKey, googleCseID, num=numWebDocs)):\n",
    "        try:\n",
    "            webArticleContent[i + 1] = extractArticle(link)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    articleData = {}\n",
    "    \n",
    "    if putTextInFile(articleContent, pathToStanCoreNLP):\n",
    "        runSub(pathToStanCoreNLP)\n",
    "        NLPppn, doc, numSenten = replaceCorefs(\"out.txt.json\", pathToStanCoreNLP)\n",
    "        doc = cleanDoc(doc)\n",
    "\n",
    "        properNouns = list(set(properNouns + NLPppn))\n",
    "        try:\n",
    "            wikiProperNoun = getAnchorTags(properNouns)\n",
    "        except Exception as e:\n",
    "            # Disambiguation error, timeout error\n",
    "            wikiProperNoun = []\n",
    "        properNouns = list(set(properNouns + wikiProperNoun))\n",
    "\n",
    "        articleData[0] = [properNouns, doc, numSenten]\n",
    "\n",
    "        for idx, content in webArticleContent.items():\n",
    "            print (idx)\n",
    "            if putTextInFile(content, pathToStanCoreNLP):\n",
    "                print (idx)\n",
    "                runSub(pathToStanCoreNLP)\n",
    "                print (idx)\n",
    "                NLPppn, doc, numSenten = replaceCorefs(\"out.txt.json\", pathToStanCoreNLP)\n",
    "                print (idx)\n",
    "                try:\n",
    "                    wikiProperNoun = getAnchorTags(NLPppn)\n",
    "                except:\n",
    "                    wikiProperNoun = []\n",
    "                print (idx)\n",
    "                properNouns = list(set(NLPppn + wikiProperNoun))\n",
    "                print (idx)\n",
    "                articleData[idx] = [properNouns, doc, numSenten]\n",
    "    else:\n",
    "        print (\"FAIL\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
